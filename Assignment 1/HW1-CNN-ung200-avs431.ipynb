{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_valid_loader(batch_size,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True,\n",
    "                           show_sample=False,\n",
    "                           num_workers=4,\n",
    "                           pin_memory=False):\n",
    "    \n",
    "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "    \n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    valid_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./cifardata', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    validset = torchvision.datasets.CIFAR10(root='./cifardata', train=True,\n",
    "                                        download=True, transform=valid_transform)\n",
    "\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    #print(\"from get loaders: \", len(train_idx))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        validset, batch_size=batch_size, sampler=valid_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return (train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 1.821\n",
      "[1,  4000] loss: 1.495\n",
      "[1,  6000] loss: 1.374\n",
      "[1,  8000] loss: 1.289\n",
      "[1, 10000] loss: 1.255\n",
      "[2,  2000] loss: 1.109\n",
      "[2,  4000] loss: 1.105\n",
      "[2,  6000] loss: 1.049\n",
      "[2,  8000] loss: 1.041\n",
      "[2, 10000] loss: 1.039\n",
      "[3,  2000] loss: 0.955\n",
      "[3,  4000] loss: 0.959\n",
      "[3,  6000] loss: 0.963\n",
      "[3,  8000] loss: 0.943\n",
      "[3, 10000] loss: 0.944\n",
      "[4,  2000] loss: 0.881\n",
      "[4,  4000] loss: 0.866\n",
      "[4,  6000] loss: 0.868\n",
      "[4,  8000] loss: 0.888\n",
      "[4, 10000] loss: 0.897\n",
      "[5,  2000] loss: 0.813\n",
      "[5,  4000] loss: 0.823\n",
      "[5,  6000] loss: 0.855\n",
      "[5,  8000] loss: 0.863\n",
      "[5, 10000] loss: 0.860\n",
      "[6,  2000] loss: 0.799\n",
      "[6,  4000] loss: 0.807\n",
      "[6,  6000] loss: 0.791\n",
      "[6,  8000] loss: 0.811\n",
      "[6, 10000] loss: 0.822\n",
      "[7,  2000] loss: 0.757\n",
      "[7,  4000] loss: 0.775\n",
      "[7,  6000] loss: 0.755\n",
      "[7,  8000] loss: 0.777\n",
      "[7, 10000] loss: 0.800\n",
      "[8,  2000] loss: 0.711\n",
      "[8,  4000] loss: 0.750\n",
      "[8,  6000] loss: 0.744\n",
      "[8,  8000] loss: 0.754\n",
      "[8, 10000] loss: 0.777\n",
      "[9,  2000] loss: 0.692\n",
      "[9,  4000] loss: 0.710\n",
      "[9,  6000] loss: 0.731\n",
      "[9,  8000] loss: 0.756\n",
      "[9, 10000] loss: 0.765\n",
      "[10,  2000] loss: 0.673\n",
      "[10,  4000] loss: 0.703\n",
      "[10,  6000] loss: 0.709\n",
      "[10,  8000] loss: 0.729\n",
      "[10, 10000] loss: 0.730\n",
      "Finished Training\n",
      "Validation Accuracy of the network on the 5000 validation images: 69 %\n",
      "Total Validation Number:  5000\n",
      "Correct Validation Number:  3492\n",
      "Validation Accuracy of the network on the 10000 Test images: 69 %\n",
      "Total Test Number:  10000\n",
      "Correct Test Number:  6924\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=50000,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 18, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(18, 24, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(24, 32, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "#         self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "#         self.fc3 = nn.Linear(256, 512)\n",
    "#         self.fc4 = nn.Linear(512, 1024)\n",
    "#         self.fc5 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print(\"X size after conv1: \", x.size())\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        #print(\"X size after conv2: \", x.size())\n",
    "#         x = self.pool3(F.relu(self.conv3(x)))\n",
    "#         x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 32*4*4)\n",
    "        #print(\"X size after flatten: \", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"X size after fc1: \", x.size())\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.fc4(x))\n",
    "        #print(\"X size after fc2: \", x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"X size after fc3: \", x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "## Getting 2 different loaders for train set and validation sets\n",
    "train_loader, valid_loader = get_train_valid_loader(4,1,0.1,False,False,4,False)\n",
    "\n",
    "#trainloader2 = torch.utils.data.DataLoader(X_train, batch_size=4,shuffle=True, num_workers=2)\n",
    "    \n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Train on batches, Test on train / validation sets\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "#outputs = net(images_test)\n",
    "\n",
    "#max_val, predicted = torch.max(outputs, 1)\n",
    "##################### VALIDATION SET #################################\n",
    "\n",
    "correct_valid = 0\n",
    "total_valid = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        max_val, predicted = torch.max(outputs.data, 1)\n",
    "        total_valid += labels.size(0)\n",
    "        correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "print('Validation Accuracy of the network on the 5000 validation images: %d %%' % (\n",
    "    100 * correct_valid / total_valid))\n",
    "print(\"Total Validation Number: \", total_valid)\n",
    "print(\"Correct Validation Number: \", correct_valid)\n",
    "\n",
    "##################### TEST SET #################################\n",
    "\n",
    "dataiter_testset = iter(testloader)\n",
    "images_testset, labels_testset = dataiter_testset.next()\n",
    "\n",
    "# outputs_testset = net(images_testset)\n",
    "# max_val, predicted = torch.max(outputs_testset, 1)\n",
    "\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "final_test_output_list = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        max_val, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        for item in predicted:\n",
    "            final_test_output_list.append(item.item())\n",
    "    print('Validation Accuracy of the network on the 10000 Test images: %d %%' % (100 * correct_test / total_test))\n",
    "    print(\"Total Test Number: \", total_test)\n",
    "    print(\"Correct Test Number: \", correct_test)\n",
    "\n",
    "\n",
    "##################### SAVE PREDICTIONS #################################\n",
    "# Save Predictions as numpy array\n",
    "final_test_output_np = np.asarray(final_test_output_list)\n",
    "filename = \"ans2-v6-ung200-avs431\"\n",
    "save_predictions(filename, final_test_output_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parameter Tuning:\n",
    "1. We also tried SGD for the same setting which gave us an accuracy of 58%\n",
    "2. We tried using RMS prop as our optimizer with Learning Rate 0.001 but for 10 epochs but it gave us an accuracy of 60%\n",
    "3. In Adam's Optimizer, we first tried increasing the weight decay to 0.05 but our cost was constant and was not decreasing even after 5 epochs so we stopped it. We also tried increasing the learning rate to 0.05 but even that didn't help.\n",
    "4. We increased the number of channels and kernel sizes but got an accuracy of 61%\n",
    "5. We tried increasing the number of neurons after the convulation is being done, and reached 63%\n",
    "6. After which we tried combining 4 and 5, so we increased the number of neurons, added a few convolutional layers and calculated and changed the kernel sizes and number of channels to get the accuracy of 69%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DATASET SIZES:\n",
    "### TRAIN: 45000\n",
    "### VALIDATION: 5000\n",
    "### TEST: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
